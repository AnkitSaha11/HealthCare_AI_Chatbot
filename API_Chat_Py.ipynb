{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9884ea64-5489-4408-9c5b-68315b181c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8afaad-6a7d-4d60-9156-a53404e2337c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d4e1b4-fae8-4903-969a-dfa8be6db77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple tokenization function that doesn't require punkt\"\"\"\n",
    "    # Split on whitespace and remove empty strings\n",
    "    tokens = [token.strip() for token in text.split()]\n",
    "    # Remove punctuation from tokens\n",
    "    tokens = [token.strip('.,!?:;()[]{}\"\"\\'') for token in tokens if token.strip('.,!?:;()[]{}\"\"\\'')]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6751f85-a6c3-4489-9569-bdf464516330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_medical_qa_pairs_with_groq(api_key, num_pairs=100, topics=None, output_json_path=\"groq_generated_qa.json\"):\n",
    "    \"\"\"\n",
    "    Generate medical Q&A pairs using Groq API\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Your Groq API key\n",
    "        num_pairs (int): Number of Q&A pairs to generate\n",
    "        topics (list): Optional list of medical topics to focus on\n",
    "        output_json_path (str): Path to save the generated Q&A pairs\n",
    "    \n",
    "    Returns:\n",
    "        list: The generated Q&A pairs\n",
    "    \"\"\"\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    # Default medical topics if none provided\n",
    "    if not topics:\n",
    "        topics = [\n",
    "            \"diabetes\", \"hypertension\", \"asthma\", \"cancer\", \"heart disease\",\n",
    "            \"arthritis\", \"allergies\", \"depression\", \"anxiety\", \"pneumonia\",\n",
    "            \"influenza\", \"COVID-19\", \"pregnancy\", \"pediatrics\", \"geriatrics\",\n",
    "            \"dermatology\", \"neurology\", \"cardiology\", \"gastroenterology\", \"hematology\"\n",
    "        ]\n",
    "    \n",
    "    all_qa_pairs = []\n",
    "    pairs_per_request = 5  # Number of pairs to generate in each API call\n",
    "    \n",
    "    print(f\"Generating {num_pairs} medical Q&A pairs using Groq API...\")\n",
    "    \n",
    "    # Calculate number of API calls needed\n",
    "    num_requests = (num_pairs + pairs_per_request - 1) // pairs_per_request\n",
    "    \n",
    "    for i in tqdm(range(num_requests)):\n",
    "        # Select random topics for this batch\n",
    "        selected_topics = np.random.choice(topics, size=min(pairs_per_request, len(topics)), replace=False)\n",
    "        topics_str = \", \".join(selected_topics)\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = f\"\"\"Generate {pairs_per_request} medically accurate question-answer pairs about the following topics: {topics_str}.\n",
    "\n",
    "For each pair:\n",
    "1. Create a detailed medical question that a patient might ask\n",
    "2. Provide a comprehensive, accurate answer based on current medical knowledge\n",
    "3. Make sure the answer is informative and would be helpful to a patient\n",
    "\n",
    "Format the output as a JSON array where each object has 'question' and 'answer' fields. Example format:\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"What are the early warning signs of type 2 diabetes?\",\n",
    "    \"answer\": \"Early warning signs of type 2 diabetes include increased thirst, frequent urination, unexplained weight loss, fatigue, blurred vision, slow-healing sores, and frequent infections. These symptoms occur because excess glucose in the bloodstream pulls fluid from tissues, making you feel thirsty, and leads to increased urination. If you experience these symptoms, consult a healthcare provider for proper testing and diagnosis.\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Important: Only output valid JSON that can be parsed. Do not include any additional text before or after the JSON.\n",
    "\"\"\"\n",
    "\n",
    "        # Prepare the API request\n",
    "        data = {\n",
    "            \"model\": \"llama3-70b-8192\",  # You can change the model as needed\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 4000,\n",
    "            \"response_format\": {\"type\": \"json_object\"}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make the API call\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            qa_pairs = json.loads(content)\n",
    "            \n",
    "            # The response might be formatted differently depending on the model's output\n",
    "            # Check if we have a direct array or if there's a nested structure\n",
    "            if isinstance(qa_pairs, list):\n",
    "                generated_pairs = qa_pairs\n",
    "            elif isinstance(qa_pairs, dict) and 'pairs' in qa_pairs:\n",
    "                generated_pairs = qa_pairs['pairs']\n",
    "            else:\n",
    "                # Try to find any array in the response\n",
    "                for key, value in qa_pairs.items():\n",
    "                    if isinstance(value, list) and len(value) > 0 and 'question' in value[0]:\n",
    "                        generated_pairs = value\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(\"Unexpected response format from Groq API\")\n",
    "            \n",
    "            # Process each pair to add token information\n",
    "            for j, pair in enumerate(generated_pairs):\n",
    "                if 'question' not in pair or 'answer' not in pair:\n",
    "                    continue\n",
    "                    \n",
    "                question = pair['question']\n",
    "                answer = pair['answer']\n",
    "                \n",
    "                # Tokenize questions and answers\n",
    "                question_tokens = simple_tokenize(question)\n",
    "                answer_tokens = simple_tokenize(answer)\n",
    "                \n",
    "                # Remove stopwords (optional)\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                question_filtered = [w for w in question_tokens if w.lower() not in stop_words]\n",
    "                answer_filtered = [w for w in answer_tokens if w.lower() not in stop_words]\n",
    "                \n",
    "                # Create entry for this QA pair\n",
    "                entry = {\n",
    "                    \"id\": len(all_qa_pairs) + j,\n",
    "                    \"question\": {\n",
    "                        \"text\": question,\n",
    "                        \"tokens\": question_tokens,\n",
    "                        \"filtered_tokens\": question_filtered\n",
    "                    },\n",
    "                    \"answer\": {\n",
    "                        \"text\": answer,\n",
    "                        \"tokens\": answer_tokens,\n",
    "                        \"filtered_tokens\": answer_filtered\n",
    "                    },\n",
    "                    \"source\": \"groq_api\",\n",
    "                    \"topics\": selected_topics.tolist()\n",
    "                }\n",
    "                \n",
    "                all_qa_pairs.append(entry)\n",
    "            \n",
    "            # Add a small delay between requests to avoid rate limiting\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in request {i+1}: {e}\")\n",
    "            time.sleep(5)  # Longer delay after an error\n",
    "    \n",
    "    # Trim to the requested number of pairs\n",
    "    all_qa_pairs = all_qa_pairs[:num_pairs]\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_qa_pairs, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Successfully generated {len(all_qa_pairs)} QA pairs\")\n",
    "    print(f\"Output saved to: {output_json_path}\")\n",
    "    \n",
    "    return all_qa_pairs\n",
    "\n",
    "def answer_question_with_groq(question, api_key, context=None, conversation_history=None):\n",
    "    \"\"\"\n",
    "    Answer a question using Groq API\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        api_key (str): Your Groq API key\n",
    "        context (str, optional): Additional context to provide to the model\n",
    "        conversation_history (list, optional): Previous conversation messages\n",
    "        \n",
    "    Returns:\n",
    "        dict: Result with answer and metadata\n",
    "    \"\"\"\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    # Prepare messages\n",
    "    messages = []\n",
    "    \n",
    "    # Add system message with context if provided\n",
    "    if context:\n",
    "        messages.append({\n",
    "            \"role\": \"system\", \n",
    "            \"content\": f\"You are a medical assistant providing accurate and helpful information. Use the following context to inform your answers: {context}\"\n",
    "        })\n",
    "    else:\n",
    "        messages.append({\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a medical assistant providing accurate and helpful information based on current medical knowledge.\"\n",
    "        })\n",
    "    \n",
    "    # Add conversation history if provided\n",
    "    if conversation_history:\n",
    "        messages.extend(conversation_history)\n",
    "    \n",
    "    # Add the current question\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "    # Prepare the API request\n",
    "    data = {\n",
    "        \"model\": \"llama3-70b-8192\",  # You can change the model as needed\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 2048\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Make the API call\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        answer = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'source': 'groq_api',\n",
    "            'processing_time': processing_time,\n",
    "            'context_used': bool(context),\n",
    "            'model': result.get('model', 'llama3-70b-8192')\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error with Groq API: {e}\")\n",
    "        return {\n",
    "            'error': True,\n",
    "            'message': str(e),\n",
    "            'answer': f\"Error getting answer: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3497c610-9dd6-423c-bcd6-66a3160f1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_medical_qa_session(api_key, qa_data=None):\n",
    "    \"\"\"\n",
    "    Run an interactive Q&A session using Groq API\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Your Groq API key\n",
    "        qa_data (list, optional): Previously generated Q&A pairs for context\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Medical Q&A Interactive Session ===\")\n",
    "    print(\"Ask medical questions (type 'quit' to exit)\")\n",
    "    \n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        # Get question from user\n",
    "        user_question = input(\"\\nYour question: \")\n",
    "        \n",
    "        if user_question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Ending session. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Try to find relevant context from previously generated QA pairs\n",
    "        context = None\n",
    "        if qa_data:\n",
    "            # Simple keyword matching for demonstration\n",
    "            keywords = set(simple_tokenize(user_question.lower()))\n",
    "            best_match_score = 0\n",
    "            best_match = None\n",
    "            \n",
    "            for qa_pair in qa_data:\n",
    "                question_keywords = set(qa_pair['question']['filtered_tokens'])\n",
    "                match_score = len(keywords.intersection(question_keywords))\n",
    "                \n",
    "                if match_score > best_match_score:\n",
    "                    best_match_score = match_score\n",
    "                    best_match = qa_pair\n",
    "            \n",
    "            if best_match and best_match_score > 0:\n",
    "                context = f\"Reference Q&A: Question: '{best_match['question']['text']}' Answer: '{best_match['answer']['text']}'\"\n",
    "        \n",
    "        # Get answer from Groq\n",
    "        print(\"Getting answer...\")\n",
    "        result = answer_question_with_groq(\n",
    "            user_question, \n",
    "            api_key, \n",
    "            context=context,\n",
    "            conversation_history=conversation_history\n",
    "        )\n",
    "        \n",
    "        if 'error' in result and result['error']:\n",
    "            print(f\"Error: {result['message']}\")\n",
    "            continue\n",
    "        \n",
    "        # Print the answer\n",
    "        print(f\"\\nAnswer (processed in {result['processing_time']:.2f}s):\")\n",
    "        print(result['answer'])\n",
    "        \n",
    "        # Update conversation history (keeping last 5 exchanges to manage context window)\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_question})\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": result['answer']})\n",
    "        \n",
    "        # Keep only the last 10 messages (5 exchanges)\n",
    "        if len(conversation_history) > 10:\n",
    "            conversation_history = conversation_history[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49fb76fa-f920-42c5-a4c7-8510647c6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(json_path):\n",
    "    \"\"\"Load the processed QA data from JSON file\"\"\"\n",
    "    print(f\"Loading data from: {json_path}\")\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87d7918c-2d20-4737-8b39-08ffec90e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qa_retrieval_model(data, model_dir=\"qa_model\"):\n",
    "    \"\"\"Build a simple retrieval-based QA model using TF-IDF\"\"\"\n",
    "    \n",
    "    # Extract questions and answers\n",
    "    questions = [item['question']['text'] for item in data]\n",
    "    answers = [item['answer']['text'] for item in data]\n",
    "    \n",
    "    # Create TF-IDF vectorizer for questions\n",
    "    print(\"Training TF-IDF vectorizer on questions...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    question_vectors = tfidf_vectorizer.fit_transform(questions)\n",
    "    \n",
    "    # Create directory for saving model artifacts\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the vectorizer\n",
    "    with open(os.path.join(model_dir, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    \n",
    "    # Save the question vectors\n",
    "    with open(os.path.join(model_dir, 'question_vectors.pkl'), 'wb') as f:\n",
    "        pickle.dump(question_vectors, f)\n",
    "    \n",
    "    # Save the answers for retrieval\n",
    "    with open(os.path.join(model_dir, 'answers.pkl'), 'wb') as f:\n",
    "        pickle.dump(answers, f)\n",
    "    \n",
    "    print(f\"Model saved to directory: {model_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'vectorizer': tfidf_vectorizer,\n",
    "        'question_vectors': question_vectors,\n",
    "        'answers': answers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c42bf135-08cf-4339-aea8-70801b246bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa_model(model, data, test_size=0.2):\n",
    "    \"\"\"Evaluate the QA model using a test set\"\"\"\n",
    "    \n",
    "    # Extract questions and answers\n",
    "    questions = [item['question']['text'] for item in data]\n",
    "    answers = [item['answer']['text'] for item in data]\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_questions, test_questions, train_answers, test_answers = train_test_split(\n",
    "        questions, answers, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Get model components\n",
    "    vectorizer = model['vectorizer']\n",
    "    train_question_vectors = vectorizer.transform(train_questions)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    correct = 0\n",
    "    total = len(test_questions)\n",
    "    \n",
    "    print(f\"Evaluating on {total} test questions...\")\n",
    "    \n",
    "    for i, test_question in enumerate(test_questions):\n",
    "        # Vectorize the test question\n",
    "        test_vector = vectorizer.transform([test_question])\n",
    "        \n",
    "        # Calculate similarity with all training questions\n",
    "        similarities = cosine_similarity(test_vector, train_question_vectors).flatten()\n",
    "        \n",
    "        # Get the most similar question's index\n",
    "        most_similar_idx = np.argmax(similarities)\n",
    "        \n",
    "        # Retrieve the corresponding answer\n",
    "        predicted_answer = train_answers[most_similar_idx]\n",
    "        \n",
    "        # Check if the answer is correct (simple string match)\n",
    "        if predicted_answer.lower() == test_answers[i].lower():\n",
    "            correct += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{total} test questions\")\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def answer_question(question, model_dir=\"qa_model\"):\n",
    "    \"\"\"Use the trained model to answer a new question\"\"\"\n",
    "    \n",
    "    # Load model components\n",
    "    with open(os.path.join(model_dir, 'tfidf_vectorizer.pkl'), 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    \n",
    "    with open(os.path.join(model_dir, 'question_vectors.pkl'), 'rb') as f:\n",
    "        question_vectors = pickle.load(f)\n",
    "    \n",
    "    with open(os.path.join(model_dir, 'answers.pkl'), 'rb') as f:\n",
    "        answers = pickle.load(f)\n",
    "    \n",
    "    # Vectorize the new question\n",
    "    question_vector = vectorizer.transform([question])\n",
    "    \n",
    "    # Calculate similarity with all known questions\n",
    "    similarities = cosine_similarity(question_vector, question_vectors).flatten()\n",
    "    \n",
    "    # Get top 3 most similar questions' indices\n",
    "    top_indices = similarities.argsort()[-3:][::-1]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'similarity': similarities[idx],\n",
    "            'answer': answers[idx]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f9ffa28-deea-440d-b57b-eb1423fb3733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Q&A System Using Groq API\n",
      "1. Generate medical Q&A pairs\n",
      "2. Interactive Q&A session\n",
      "3. Generate Q&A pairs and then start interactive session\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select an option (1/2/3):  1\n",
      "How many Q&A pairs to generate? (default: 50):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1 medical Q&A pairs using Groq API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in request 1: 400 Client Error: Bad Request for url: https://api.groq.com/openai/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 0 QA pairs\n",
      "Output saved to: groq_generated_qa.json\n",
      "\n",
      "Example generated Q&A pair:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual Groq API key\n",
    "    GROQ_API_KEY = \"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\"\n",
    "    \n",
    "    # Ask if user wants to generate new QA pairs or use interactive mode\n",
    "    print(\"Medical Q&A System Using Groq API\")\n",
    "    print(\"1. Generate medical Q&A pairs\")\n",
    "    print(\"2. Interactive Q&A session\")\n",
    "    print(\"3. Generate Q&A pairs and then start interactive session\")\n",
    "    \n",
    "    choice = input(\"Select an option (1/2/3): \")\n",
    "    \n",
    "    qa_data = None\n",
    "    \n",
    "    if choice == '1' or choice == '3':\n",
    "        # Ask how many QA pairs to generate\n",
    "        num_pairs = int(input(\"How many Q&A pairs to generate? (default: 50): \") or \"50\")\n",
    "        \n",
    "        # Generate Q&A pairs\n",
    "        qa_data = generate_medical_qa_pairs_with_groq(\n",
    "            GROQ_API_KEY, \n",
    "            num_pairs=num_pairs,\n",
    "            output_json_path=\"groq_generated_qa.json\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\nExample generated Q&A pair:\")\n",
    "        if qa_data:\n",
    "            example = qa_data[0]\n",
    "            print(f\"Question: {example['question']['text']}\")\n",
    "            print(f\"Answer: {example['answer']['text']}\")\n",
    "    \n",
    "    if choice == '2' or choice == '3':\n",
    "        # If we didn't generate QA pairs but the file exists, load it\n",
    "        if not qa_data and os.path.exists(\"groq_generated_qa.json\"):\n",
    "            try:\n",
    "                with open(\"groq_generated_qa.json\", 'r', encoding='utf-8') as f:\n",
    "                    qa_data = json.load(f)\n",
    "                print(f\"Loaded {len(qa_data)} previously generated Q&A pairs for context\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading previous Q&A pairs: {e}\")\n",
    "        \n",
    "        # Run interactive session\n",
    "        interactive_medical_qa_session(GROQ_API_KEY, qa_data=qa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057f735-78af-49c5-b824-334e57e704a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
